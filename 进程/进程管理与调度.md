# 进程管理与调度

## 进程优先级

1. **硬实时进程**：某些任务必须在指定时限内完成。关键特征是在可保证的时间范围内得到处理。Linux不支持硬实时处理，但是修改版本，比如RTLinux，Xenomai，RATI提供该特定。在降低内核整体延时方面的相关工作包括：可抢占的内核机制、实时互斥量、完全公平的新调度器
2. **软实时进程**：时限是一个柔性灵活的，可以容忍偶然的超时错误。如对CD的写入，如果系统负荷过高，数据流可能会暂时中断，不过需要的CPU时间应该能够保证，至少优先于所有哦其他普通进程。
3. **普通进程** ：没有特定时间约束，仍然可以根据重要性来分配优先级。

进程运行按照时间片调度，分配给进程的时间片份额与其重要性相当。系统时间的流动对应于圆盘的转动，CPU则由圆周旁的扫描器表示。最终，重要的进程比次要的得到更多的CPU时间。

![img](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/20170110150657321.png?raw=true)

### 调度两次重写

1. **O(1) 调度器**代替前一个调度器，该调度器一个特别的性质是，它可以在常数时间内完成工作，不依赖于系统上运行的进程数目。
2. **完全公平调度器** 该调度器的关键特性是：它试图尽可能地模仿理想情况下的公平调度。还能够处理更一般性的调度实体，例如该调度器分配可用时间时，可以首先在不同用户之间分配，接下来在各个用户的进程之间分配。

## 进程生命周期

### 进程状态

- 运行：该进程此时正在运行
- 等待：进程能够运行，但是没有得到许可。因为CPU分配给另一个进程。调度器可以在下一次任务切换时，选择该进程。
- 睡眠：进程正在睡眠无法运行，因为它在等待一个外部事件。调度器无法在下一次任务切换时选择该进程

### 进程状态切换(生命周期)

![img](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/68747470733a2f2f706963332e7a68696d672e636f6d2f76322d62633337376435366533316630323436363466333837303638646334376261655f622e6a7067.jpg?raw=true)

内核调整进程状态：

- TASK_RUNNING（运行）—说明进程是可执行的：它或者正在执行，或者在运行队列中待执行。这是进程在用户空间中执行的唯一可能状态。
- TASK_INTERRUPTIBLE（可中断）—线程正在睡眠（被阻塞），等待某些条件的达成。一旦条件达成，内核就会把进程状态设置为运行。该状态可能会因为接收到信号而提前被唤醒并准备投入运行。
- TASK_UNINTERRUPTIBLE（不可中断）—与可中断状态相比，不同点在于此状态下的进程即使收到信号也不会被唤醒。使用较少。
- TASK_TRACED—被其他进程追踪的进程。
- TASK_STOPPED（停止）—进程停止执行：进程没有投入运行也不能被投入运行。

应该注意以下内容：

- 只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。
- 阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。
- 进程只能自己阻塞自己，因为只有进程自身才知道何时需要等待某种事件的发生

### 僵尸状态

在两种事件发生时，程序将终止运行

1. 程序必须由另一个进程或一个用户杀死(通常是通过发送SIGTERM或SIGKILL信号完成)，
2. 进程的父进程在子进程终止时，必须调用或已经调用wait4系统调用，这相当于向内核证明父进程已经确认子进程的终结

**僵尸状态**：只有在第一个条件发生(程序终止)而第二个条件不成立的情况下(wait4)，才会出现。从进程工具(ps/top)输出，可以看到僵尸进程。

### 抢占式多任务处理

两种进程状态：**用户态和核心态**，其中一种权利受到各种限制，另一种具有无限的权利。

通常进程处于用户态，只能访问自身数据，无法干扰其他应用程序。如果想要访问系统数据或功能，则必须切换到核心态。

- 系统调用
- 中断

内核的抢占调度模型建立一个层次结构，用于判断哪些进程状态可以由其他状态抢占

- 普通进程总是可能被抢占，甚至是由其他进程抢占
- 系统处于核心态，并且正在处理系统调用，其他进程无法夺取其CPU时间。必须等到系统调用执行完成，才能选择另一个进程执行，但中断可以终止系统调用
- 中断可以暂停处于用户状态和核心状态的进程。中断具有最高优先级，因为在中断触发后需要尽快处理

## 进程表示

Linux内核涉及进程和程序的所有算法都围绕一个名为task_struct的数据结构建立，定义在include/sched.h中。

### task_struct 定义

```c
#ifdef CONFIG_THREAD_INFO_IN_TASK
	struct thread_info		thread_info;
#endif
	/* -1 unrunnable, 0 runnable, >0 stopped: */
	volatile long			state;

	randomized_struct_fields_start

	void				*stack;
	refcount_t			usage;
	/* Per task flags (PF_*), defined further below: */
	unsigned int			flags;
	unsigned int			ptrace;

#ifdef CONFIG_SMP
	struct llist_node		wake_entry;
	int				on_cpu;
#ifdef CONFIG_THREAD_INFO_IN_TASK
	/* Current CPU: */
	unsigned int			cpu;
#endif
	unsigned int			wakee_flips;
	unsigned long			wakee_flip_decay_ts;
	struct task_struct		*last_wakee;

	int				recent_used_cpu;
	int				wake_cpu;
#endif
	int				on_rq;

	int				prio;
	int				static_prio;
	int				normal_prio;
	unsigned int			rt_priority;

	const struct sched_class	*sched_class;
	struct sched_entity		se;
	struct sched_rt_entity		rt;
#ifdef CONFIG_CGROUP_SCHED
	struct task_group		*sched_task_group;
#endif
	struct sched_dl_entity		dl;

#ifdef CONFIG_UCLAMP_TASK
	/* Clamp values requested for a scheduling entity */
	struct uclamp_se		uclamp_req[UCLAMP_CNT];
	/* Effective clamp values used for a scheduling entity */
	struct uclamp_se		uclamp[UCLAMP_CNT];
#endif

#ifdef CONFIG_PREEMPT_NOTIFIERS
	/* List of struct preempt_notifier: */
	struct hlist_head		preempt_notifiers;
#endif

#ifdef CONFIG_BLK_DEV_IO_TRACE
	unsigned int			btrace_seq;
#endif

	unsigned int			policy;
	int				nr_cpus_allowed;
	const cpumask_t			*cpus_ptr;
	cpumask_t			cpus_mask;

#ifdef CONFIG_PREEMPT_RCU
	int				rcu_read_lock_nesting;
	union rcu_special		rcu_read_unlock_special;
	struct list_head		rcu_node_entry;
	struct rcu_node			*rcu_blocked_node;
#endif /* #ifdef CONFIG_PREEMPT_RCU */

#ifdef CONFIG_TASKS_RCU
	unsigned long			rcu_tasks_nvcsw;
	u8				rcu_tasks_holdout;
	u8				rcu_tasks_idx;
	int				rcu_tasks_idle_cpu;
	struct list_head		rcu_tasks_holdout_list;
#endif /* #ifdef CONFIG_TASKS_RCU */

	struct sched_info		sched_info;

	struct list_head		tasks;
#ifdef CONFIG_SMP
	struct plist_node		pushable_tasks;
	struct rb_node			pushable_dl_tasks;
#endif

https://github.com/torvalds/linux/blob/master/include/linux/sched.h
```

该结构的内容可以分解为各个部分

- 状态和信息：如待决信号，使用的二进制，进程ID号(pid)，到父进程及其他有关进程的指针，优先级和程序执行时间信息
- 有关已经分配的虚拟内存信息
- 进程身份凭证，如用户ID，组ID以及权限
- 使用的文件包含程序的二进制文件，以及进程所处理的所有文件的文件系统信息
- 线程信息记录该进程特定于CPU的运行时间数据
- 该进程所用的信号处理程序，用于响应到来的信号

### task_struct 重要成员

state指定了进程的当前状态

- TASK_RUNNING表示进程要么正在执行，要么正要准备执行。
- TASK_INTERRUPTIBLE表示进程被阻塞（睡眠），直到某个条件变为真。条件一旦达成，进程的状态就被设置为TASK_RUNNING。
- TASK_UNINTERRUPTIBLE的意义与TASK_INTERRUPTIBLE类似，除了不能通过接受一个信号来唤醒以外。
- TASK_STOPPED表示进程被停止执行。
- TASK_TRACED表示进程被debugger等进程监视。

表示退出状态，EXIT_ZOMBIE和EXIT_DEAD也可以存放在exit_state成员中。

- EXIT_ZOMBIE表示进程的执行被终止，但是其父进程还没有使用wait()等系统调用来获知它的终止信息。
- EXIT_DEAD表示进程的最终状态。

### 资源限制

Linux提供资源限制机制，对进程使用系统资源施加某些限制，该机制利用task_struck中的rlim数组。

rlim可能的常数及含义(特定于进程的资源限制)：	

|        常数         |           语义           |
| :---------------: | :--------------------: |
|    RLIMIT_CPU     |     按毫秒计算的最大CPU时间      |
|   RLIMIT_FSIZE    |       允许的最大文件长度        |
|    RLIMIT_DATA    |        数据段的最大长度        |
|   RLIMIT_STACK    |      (用户状态)栈的最大长度      |
|    RLIMIT_CORE    |      内存转储文件的最大长度       |
|    RLIMIT_RSS     | 常驻内存的最大尺寸(进程使用页帧的最大数目) |
|   RLIMIT_NPROC    |  与进程UID关联的用户拥有进程最大数目   |
|   RLIMIT_NOFILE   |       打开文件的最大数目        |
|  RLIMIT_MEMLOCK   |       不可换出页的最大数目       |
|     RLIMIT_AS     |    进程占用的虚拟地址空间最大尺寸     |
|   RLIMIT_LOCKS    |        文件锁的最大数目        |
| RLIMIT_SIGPENDING |       待决信号的最大数目        |
|  RLIMIT_MSGQUEUE  |       信息队列的最大数目        |
|    RLIMIT_NICE    |       非实时进程的优先级        |
|   RLIMIT_RTPRIO   |        最大的实时优先级        |

如果某一类资源没有使用限制，则将rlim_max设置为RLIM_INFINITY。

### 进程类型

典型的UNIX进程包括：由二进制代码组成的应用程序，单线程，分配给应用程序的一组资源(内存、文件等)。新进程使用fork和exec系统调用产生。

- fork生成当前进程的一个副本，称为子进程。原进程所有哦资源都复制到子进程，原来的进程就有了两个独立的实例
- exec从一个可执行的二进制文件加载另一个应用程序，来代替当前运行的进程。必须先使用fork复制一个旧的程序，然后调用exec在系统上创建另一个应用程序

### 命名空间

NameSpace是Linux提供的一种内核级别环境隔离的方法，提供了对UTS、IPC、mount、PID、network、User等的隔离机制

**分类：**

| 分类                 | 系统调用参数        | 相关内核版本                          | 隔离内容                                     |
| ------------------ | ------------- | ------------------------------- | ---------------------------------------- |
| Mount namespaces   | CLONE_NEWNS   | Linux 2.4.19                    | 挂载点（文件系统）                                |
| UTS namespaces     | CLONE_NEWUTS  | Linux 2.6.19                    | 主机名与域名,影响uname(hostname, domainname)     |
| IPC namespaces     | CLONE_NEWIPC  | Linux 2.6.19                    | 信号量、消息队列和共享内存, inter-process communication，有全局id |
| PID namespaces     | CLONE_NEWPID  | Linux 2.6.24                    | 进程编号                                     |
| Network namespaces | CLONE_NEWNET  | 始于Linux 2.6.24 完成于 Linux 2.6.29 | 网络设备、网络栈、端口等等                            |
| User namespaces    | CLONE_NEWUSER | 始于 Linux 2.6.23 完成于 Linux 3.8)  | 用户和用户组                                   |

**三种系统调用：**

| 调用        | 作用                                    |
| --------- | ------------------------------------- |
| clone()   | 实现线程的系统调用，用来创建一个新的进程，并可以通过设计上述参数达到隔离。 |
| unshare() | 使某进程脱离某个namespace                     |
| setns()   | 把某进程加入到某个namespace                    |

### CGroup

cgroups可以限制、记录、隔离进程组所使用的物理资源(CPU，memory，IO等)，为容器是吸纳虚拟化提供基本保证。

提供了如下功能：

- Resource limitation: 限制资源使用，比如内存使用上限以及文件系统的缓存限制。
- Prioritization: 优先级控制，比如：CPU利用和磁盘IO吞吐。
- Accounting: 一些审计或一些统计，主要目的是为了计费。
- Control: 挂起进程，恢复执行进程。

**CGroup的子系统**

- blkio 这个subsystem可以为块设备设定输入/输出限制，比如物理驱动设备（包括磁盘、固态硬盘、USB等）。
- cpu 这个subsystem使用调度程序控制task对CPU的使用。
- cpuacct 这个subsystem自动生成cgroup中task对CPU资源使用情况的报告。
- cpuset 这个subsystem可以为cgroup中的task分配独立的CPU（此处针对多处理器系统）和内存。
- devices 这个subsystem可以开启或关闭cgroup中task对设备的访问。
- freezer 这个subsystem可以挂起或恢复cgroup中的task。
- memory 这个subsystem可以设定cgroup中task对内存使用量的限定，并且自动生成这些task对内存资源使用情况的报告。
- perfevent 这个subsystem使用后使得cgroup中的task可以进行统一的性能测试。{undefined}
- net_cls 这个subsystem Docker没有直接使用，它通过使用等级识别符(classid)标记网络数据包，从而允许 Linux 流量控制程序（TC：Traffic Controller）识别从具体cgroup中生成的数据包。

**规则**

- 同一个hierarchy可以附加一个或多个subsystem
- 一个subsystem可以附加到多个hierarchy，当且仅当这些hierarchy只有这唯一一个subsystem。
- 系统每次新建一个hierarchy时，该系统上的所有task默认构成了这个新建的hierarchy的初始化cgroup，这个cgroup也称为root cgroup。
- 进程（task）在fork自身时创建的子任务（child task）默认与原task在同一个cgroup中，但是child task允许被移动到不同的cgroup中。

### 进程ID号

**进程ID号**，简称PID，用fork或clone产生的每个进程都由内核自动的分配一个新的唯一PID值。

进程ID可能的类型：

- 处于某个线程组中的所有进程都有统一的线程组ID(TGID)。如果没有使用线程，则PID和TGID相同
- 独立进程可以合并成进程组
- 几个进程组可以合并成一个会话，会话中的所有进程都有同样的会话ID

PID命名空间按层次组织，在建立一个新的命名空间时，该命名空间中的所有PID对父命名空间都是可见的，但子命名空间无法看到父命名空间的PID。这反映在数据结构中，区分为局部ID和全局ID

- 全局ID是在内核本身和初始化命名空间中的唯一ID号，保证在整个系统中是唯一的
- 局部ID属于某个特定的命名空间。对每个ID类型，它们所属的命名空间内部有效，但类型相同，值也相同的ID可能出现在不同的命名空间中


**管理PID**

## 进程管理的系统调用

### **进程复制**

Linux实现了3个进程复制

- fork是重量级调用，建立父进程一个完整的副本，然后作为子进程执行。Linux使用了写时复制(copy-on-write)技术
- vfork类似于fork，但是并不创建父进程数据副本。相反，父子之间共享数据，节省大量CPU时间
- clone产生线程，可以对父子进程之间的共享，复制进行精确控制

**写时复制**

进程通常只使用了其内存页的一小部分。在调用fork时，内核通常对父进程的每个内存页，都为子进程创建一个相同的副本。产生的负面效应：

- 使用大量内存
- 复制操作耗费很长时间

如果应用程序在进程复制之后使用exec立刻加载新程序，那么此前复制操作是完全多余的。因为进程地址空间会重新初始化，复制的数据不再需要

写时复制步骤

1. 并不复制进程的整个地址空间，而是只复制页表。建立虚拟地址空间和物理内存之间关系
2. 假如两个进程只能读取其内存页，二者之间的数据共享就不是问题，因为不会有修改
3. 其中一个进程试图向复制的内存页写入，处理器向内核报告访问错误(缺页异常)。内核查看额外的内存管理数据结构，检查该页是否可以读写模式访问
   - 只读模式，必须向进程报告段错误
   - 可写，内存会创建该页专用于当前进程的副本

**do_fork实现**

所有三个fork机制最终低啊用了kernel/fork.c中的do_fork，代码流程：

![do_fork](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/20160602203201604.png?raw=true)

在子进程生成之后，内核必须执行以下收尾操作

- 由于fork要返回新进程的PID，因此必须获得PID
- 如果将要使用Ptrace监控新的进程，那么在创建新进程后会立刻向其发送SIGSTOP信号，以便附接的调试器检查其数据
- 子进程使用wake_up_new_task唤醒，将其task_struct添加到调度器队列
- 如果使用vfork机制，必须启用子进程的完成机制(父进程在该变量上进入睡眠状态，知道子进程退出

**复制进程操作**

copy_process函数调

![copy_process](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/20160602203329356.png?raw=true)

**简化的copy_process()流程**

1. dup_task_struct()。分配一个新的进程控制块，包括新进程在kernel中的堆栈。新的进程控制块会复制父进程的进程控制块，但是因为每个进程都有一个kernel堆栈，新进程的堆栈将被设置成新分配的堆栈。
2. 初始化一些新进程的统计信息，如此进程的运行时间
3. copy_semundo()复制父进程的semaphore undo_list到子进程。
4. copy_files()、copy_fs()。复制父进程文件系统相关的环境到子进程
5. copy_sighand()、copy_signal()。复制父进程信号处理相关的环境到子进程。
6. copy_mm()。复制父进程内存管理相关的环境到子进程，包括页表、地址空间和代码数据。
7. copy_thread()/copy_thread_tls。设置子进程的执行环境，如子进程运行时各CPU寄存器的值、子进程的kernel栈的起始地址。
8. sched_fork()。设置子进程调度相关的参数，即子进程的运行CPU、初始时间片长度和静态优先级等。
9. 将子进程加入到全局的进程队列中
10. 设置子进程的进程组ID和对话期ID等。

简单的说，copy_process()就是将父进程的运行环境复制到子进程并对某些子进程特定的环境做相应的调整。

### 内核线程

内核线程是直接由内核本身启动的进程。执行以下任务

- 周期性将修改的内存页与页来源块设备同步
- 如果内存很少使用，则写入交换区
- 管理延时动作
- 实现文件系统的事务日志

有两种类型的内核线程

- 类型1：线程启动后一直等待，直至内核请求线程执行某一特定操作
- 类型2：线程启动后按周期性间隔运行，检测特定资源的使用，在用量超过或低于预置的限制值时采取行动。用于连续监测任务

调用kernel_thread函数可以启动一个内核线程

内核线程是由内核自身生成的，应该注意两点：

1. 它们在CPU的管态(supervisor mode)执行，而不是用户状态
2. 它们只可以访问虚拟地址空间的内核部分，但不能访问用户空间

### 启动新程序

通过新代码替换现存代码，就可启动新程序，Linux提供的execve系统调用。该函数将其工作委托给系统无关的do_execve历程

![do_execve函数的流程](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/20160606114852862.png?raw=true)

do_execve则是调用do_execveat_common实现的，依次执行以下操作：

1. 调用unshare_files()为进程复制一份文件表
2. 调用kzalloc()分配一份structlinux_binprm结构体
3. 调用open_exec()查找并打开二进制文件
4. 调用sched_exec()找到最小负载的CPU，用来执行该二进制文件
5. 根据获取的信息，填充structlinux_binprm结构体中的file、filename、interp成员
6. 调用bprm_mm_init()创建进程的内存地址空间，为新程序初始化内存管理.并调用init_new_context()检查当前进程是否使用自定义的局部描述符表；如果是，那么分配和准备一个新的LDT
7. 填充structlinux_binprm结构体中的argc、envc成员
8. 调用prepare_binprm()检查该二进制文件的可执行权限；最后，kernel_read()读取二进制文件的头128字节（这些字节用于识别二进制文件的格式及其他信息，后续会使用到）
9. 调用copy_strings_kernel()从内核空间获取二进制文件的路径名称
10. 调用copy_string()从用户空间拷贝环境变量和命令行参数
11. 至此，二进制文件已经被打开，struct linux_binprm结构体中也记录了重要信息, 内核开始调用exec_binprm执行可执行程序
12. 释放linux_binprm数据结构，返回从该文件可执行格式的load_binary中获得的代码

**Linux支持的二进制格式**

|        名称        |                    含义                    |
| :--------------: | :--------------------------------------: |
|   flat_format    |         flat格式用于没有内存管理单元的嵌入式CPU上         |
|  script_format   | 用于运行使用#！机制的脚本。检测文件的第一行，内核即知道使用何种解释器，启动适当的应用程序 |
|   misc_format    |     用于启动需要外部解释器的应用程序。例如用于执行Java字节代码      |
|    elf_format    |        与计算机和体系结构无关的结构，是Linux的标准格式        |
| elf_fdpic_format |         ELF格式变体，提供针对没有MMU系统的特别特性         |
|   irix_format    |           EFI格式变体，提供特定于Irix的特性           |
|   aout_format    |         a.out是引入ELF之前Linux的标准格式          |

每种二进制格式必须提供3个函数

- load_binary用于加载普通程序
- load_shlib用于加载共享库，即动态库
- core_dump用于在程序错误的情况下输出内存转储

### 退出进程

进程必须低啊用exit系统调用终止，这使内核有机会将该进程使用的资源释放回系统。

## 调度器的实现

### 完全公平调度器

内存中保存了对每个进程的唯一描述。调度器的任务是在程序之间共享CPU时间，创造并行执行的错觉。

分为两个部分：一个涉及调度策略，另一个涉及上下文切换

*注意：暂时忽略实时进程，只考虑完全公平调度器*

当前的调度器只考虑进程等待时间，即进程在就绪队列中已经等待了多长时间。

**一般原理**：一台理想计算机，可以并行运行任意数目的进程。如果系统上有N个进程，那么每个进程得到总计算能力的1/N，所有进程在物理上实现并行执行。

![img](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/L3Byb3h5L2h0dHBzL2ltZzIwMTguY25ibG9ncy5jb20vYmxvZy8xNDU5MjcyLzIwMTgxMC8xNDU5MjcyLTIwMTgxMDE5MTA0NDA5MDA0LTk3NzEwNDE5LnBuZw==.png?raw=true)

调度器通过将进程在红黑树中排序，跟踪进程的等待时间。

所有的可运行进程都按时间在一个红黑树中排序，所谓时间就是等待时间。等待CPU时间最长的进程是最左侧的项，调度器下一次会考虑该进程。等待时间稍短的进程在该树上从左到右排序。

**红黑树**

该数据结构对所包含的项提供了高效的管理。该树管理的进程数目增加时，查找、插入、删除操作需要的时间只会适度增加。红黑树是内核的标准数据结构

就绪队列还装备了虚拟时钟。该时钟的时间流逝速度慢于实际的时钟，精确的速度依赖于当前等待调度器挑选的进程数目。**假定**：该队列上有4个进程，那么虚拟时钟将以实际时钟的1/4的速度运行。如果以完全公平的方式分享计算能力，那么该时钟是判断等待进程将获得多少CPU时间的基准。在就绪队列等待实际的20秒，相当于虚拟时间5秒。4个进程分别执行5秒。

就绪队列的虚拟时间由fair_clock给出，进程的等待时间保存在wait_runtime中，为排序红黑树上的进程，内核使用差值fair_clock - wait_runtime的绝对值。fair_clock是完全公平调度的情况下进程将会得到的CPU时间的度量，而wait_runtime直接度量了实际系统的不足造成的不公平。

在进程允许运行时，将从wait_runtime减去它已经运行的时间。这样，在按时间排序的树中它会向右移动到某一点，另一个进程将成为最左边，下一次会被调度器选择。

### 数据结构

两种方法激活调度

- 直接，比如进程打算睡眠或出于其他原因放弃CPU
- 通过周期性机制，以固定的频率运行，不时检测是否有必要进行进程切换

![这里写图片描述](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/20170110150657321.png?raw=true)

1. 调度类用于判断接下来运行哪个进程。

   内核支持不同的调度策略(完全公平调度，实时调度，在无事可做时调度空闲进程)，

2. 在选中将要运行的进程之后，必须执行底层任务切换。

   每个进程都刚好属于某一调度类，各个调度类负责管理所属的进程。通用调度器自身完全不涉及进程管理，其工作都委托给调度器类

**task_struct成员**

定义

```c
 struct task_struct {
 ...
   // prio和normal_prio表示动态优先级，static_prio表示进程的静态优先级。静态优先级是进程启动时分配的优先级。它可以用nice和sched_setscheduler系统调用修改，否则在进程运行期间会一直保持恒定。normal_priority表示基于进程的静态优先级和调度策略计算出的优先级。调度器考虑的优先级则保存在prio。由于在某些情况下内核需要暂时提高进程的优先级，因此需要第3个成员来表示。
     int prio, static_prio, normal_prio; 
   
   //表示实时进程的优先级。该值不会代替先前讨论的那些值。最低的实时优先级为0，最高的是99。值越大，优先级越高。这里使用的惯例不同于nice值。
     unsigned int rt_priority; 
   
   //是循环实时调度器所需要的，但不用于完全公平调度器，run_list是一个表头，用于维护包含各进程的一个运行表
     struct list_head run_list; 
   
   //表示该进程所属的调度器类
     const struct sched_class *sched_class; 
   
   //由于调度器设计为处理可调度的实体，在调度器看来各个进程必须也像是这样的实体。因此se在task_struct中内嵌了一个sched_entity实例，调度器可据此操作各个task struct。
     struct sched_entity se; 
   
   //保存了对该进程应用的调度策略。（SCHED_NORMAL用于普通进程，SCHED_BATCH用于非交互、CPU使用密集的批处理进程，SCHED_IDLE是空闲进程，SCHED_RR和SCHED_FIFO用于实现软实时进程，）
     unsigned int policy; 
   
   //是一个位域，在多处理器系统上用来限制进程可以在哪些CPU上运行
     cpumask_t cpus_allowed; 
   
   // 是循环实时调度器所需要的，但不用于完全公平调度器，time_slice则指定进程可使用CPU的剩余时间段
     unsigned int time_slice; 
 ...
 }
```

**调度器类**

调度器类提供了通用调度器和各个调度方法之间的关联。对各个调度类，都必须提供struct sched_class的一个实例。

调度类之间的层次结构是平坦：实时进程最重要，然后完全公平进程优先于空闲进程

**就绪队列**

定义

````c
 struct rq {
     unsigned long nr_running; //指定了队列上可运行进程的数目，不考虑其优先级或调度类
 #define CPU_LOAD_IDX_MAX 5
     unsigned long cpu_load[CPU_LOAD_IDX_MAX]; //用于跟踪此前的负荷状态
 ...
     struct load_weight load; //提供了就绪队列当前负荷的度量
     struct cfs_rq cfs;  //嵌入的子就绪队列，用于完全公平调度器
     struct rt_rq rt;  //嵌入的子就绪队列，用于和实时调度器
     struct task_struct *curr, *idle; //指向idle进程的task_struct实例，该进程亦称为idle线程
     u64 clock; //用于实现就绪队列自身的时钟。每次调用周期性调度器时，都会更新clock的值。
 ...
 };
````

核心调度器用于管理活动进程的主要数据结构。

各个CPU都有自身的就绪队列，各个活动进程只出现在一个就绪队列中。进程不是由就绪队列的成员直接管理的，而是有调度类管理，就绪队列中嵌入了特定于调度器类的子就绪队列。

**调度实体**

由于调度器可以操作比进程更一般的实体，因此需要一个适当的数据结构来描述。

定义

```c
 struct sched_entity {
     struct load_weight load;  //指定了权重，用于负载均衡
     struct rb_node run_node; //是标准的树结点，使得实体可以在红黑树上排序
     unsigned int on_rq; //表示该实体当前是否在就绪队列上接受调度
     u64 exec_start; //新进程加入就绪队列时，或者周期性调度器中。每次调用时，会计算当前时间和exec_start之间的差值，exec_start则更新到当前时间。差值则被加到sum_exec_runtime。
     u64 sum_exec_runtime; //用于记录消耗的CPU时间
     u64 vruntime; //统计进程执行期间虚拟时钟上流逝的时间数量
     u64 prev_sum_exec_runtime; //进程被撤销CPU时，保存当前sum_exec_runtime
 ...
 }
```

### 处理优先级

**优先级的内核表示**

用户空间通过nice值设定静态优先级，内核使用一个简单的数值范围，[0,+139]表示内部优先级，同样是值越低，优先级越高。从0到99的范围专供实时进程使用。nice值[-20，+19]映射范围100到139。

|   优先级范围   |     描述     |
| :-------: | :--------: |
|  0 - 99   | 实时进程(高优先级) |
| 100 - 139 |   非实时进程    |

三种优先级

- 动态优先级(task_struct - > prio)
- 普通优先级(task_struct - > normal_prio)
- 静态优先级(task_struct - > static_prio)

**对各种类型的进程计算优先级**

|  进程类型/优先级   | static_prio |          normal_prio          |    prio     |
| :---------: | :---------: | :---------------------------: | :---------: |
|    非实时进程    | static_prio |          static_prio          | static_prio |
| 优先级提高的非实时进程 | static_prio |          static_prio          |   prio不变    |
|    实时进程     | static_prio | MAX_RT_PRIO - 1 - rt_priority |   prio不变    |

*注意：在进程分支出子进程，子进程的静态优先级继承自父进程。动态优先级，即task_struct - > prio 设置为父进程的普通优先级。确保实时互斥量引起的优先级提高不会传递给子进程*

**负荷权重**

进程重要性不仅由优先级指定，还需要考虑task_struct - > se.load的负荷权重。set_load_weight根据进程类型及其静态优先级计算负荷权重

一般概念是，进程每降低一个nice值，则多获得10%的CPU时间，每升高一个nice值，则放弃10%的CPU时间。为执行该策略，内核将优先级转换为权重值。

负荷权重和数值转换表

```c
static const int prio_to_weight[40] = {
  /* -20 */	88761,	71755,	56483,	46273,	36291,
  /* -15 */	29154,	23254,	18705,	14949,	11916,
  /* -10 */	9548,	7620,	6100,	4904,	3906,
  /* -5 */	3121,	2501,	1991,	1586,	1277,
  /* 0 */	1024,	820,	655,	526,	423,
  
  /* 5 */	335,	272,	215,	172,	137,
  /* 10 */	110,	87,	70,	56,	45,
  /* 15 */	36,	29,	23,	18,	15,
};
```

对内核使用的范围[0，39]中的每个nice级别，该数组都有一个对应项。个数组之间的乘数因子是1.25。

**例子**：进程A和B在nice级别0运行，因此两个CPU份额相同，都是50%。nice为0对应数值1024，每个进程份额是1024/(1024+1024) = 0.5，即50%。

如果进程B的优先级加1，那么CPU份额减少10%。A得到CPU时间的55%，B得到45%。优先级增加1导致权重减少，即1024/1.25 = 820。A得到的份额是1024/(1024+820) = 0.55。B份额是820/(1024+820) = 0.45。

不仅进程，而且就绪队列也关联到一个负荷权重。每次进程被加到就绪队列，内核会调用inc_nr_running。不仅确保就绪队列能够跟踪记录有多少进程在允许。而且还将进程的权重添加到就绪队列的权重中。

### 核心调度器

调度器实现函数，根据现有进程的优先级分配CPU时间

- 周期性调度函数
- 主调度函数

**周期性调度器**

如果系统正常活动，内核会按照频率HZ自动调用该函数。主要任务

1. 管理内核中与整个系统和各个进程的调度相关的统计量。其间执行的主要操作是对各种计数器加1
2. 激活负责当前进程的调度类的周期性调度方法

负责处理就绪队列时钟的更新。本质上相当于将数组中先前存储的负荷值向后移动一个位置，将当前就绪队列的负荷记入数组的第一个位置。另外，该函数还引入一些取平均值的技巧，以确保负荷数组的内容不会呈现出太多的不连续跳变。

**主调度器**

在内核，如果将CPU分配给与当前活动进程不同的另一个进程，都会直接调用主调度器函数(schedule)。

该函数完成如下工作

1. 确定当前就绪队列, 并在保存一个指向当前(仍然)活动进程的task_struct指针。类似于周期性调度器，内核也利用该时机来更新就绪队列时钟，并清除当前运行进程task_struct中的重要标志TIF_NEED_RESCHED。
2. 因为调度器的模块化结构，大多数工作可以委托给调度类。如果当前进程原来处于可中断睡眠状态但现在接受到信号，那么它必须再次提升为运行进程。否则，用相应调度器类的方法使进程停止活动

**与fork交互**

每当使用fork系统调用或其他方式建立新进程时，调度器有机会用sched_fork函数挂钩到该进程。

1. 初始化新进程与调度相关的字段
2. 建立数据结构
3. 确定进程的动态优先级

**上下文切换**

内核选择新进程之后，必须处理与多任务相关的技术细节。这些细节称为上下文切换。通过两个特定于处理器的函数完成。

1. switch_mm更换通过task_struct -> mm描述的内存管理上下文。

   主要包括

   - 加载页表
   - 刷出地址转换后备缓冲器
   - 向内存管理单元提供新的信息

2. switch_to切换处理器寄存器内容和内核栈。包括用户态下的栈，因此用户栈就不需要显示变更了

**TLB**：由于用户空间进程的寄存器内容进入核心态时保存在内核栈上，在上下文切换期间无需显式操作。而每个进程首先从核心态开始执行，在返回用户空间时，会使用内核栈上保存的值自动恢复寄存器数据。enter_lazy_tlb通知底层体系结构不需要切换虚拟地址空间的用户空间部分，可以加速上下文切换

**switch_to**

该函数保存原进程的所有寄存器信息，恢复新进程的所有寄存器信息，并执行新的进程

内核在switch_to中执行如下操作

1. 进程切换, 即esp的切换, 由于从esp可以找到进程的描述符
2. 硬件上下文切换, 设置ip寄存器的值, 并jmp到__switch_to函数
3. 堆栈的切换, 即ebp的切换, ebp是栈底指针, 它确定了当前用户空间属于哪个进程

**惰性FPU模式**

由于上下文切换的速度对系统性能的影响举足轻重，所以内核使用了浮点寄存器减少CPU时间。

惰性FPU技术：对于浮点寄存器，除非有应用程序实际使用，否则不会保存；除非有应用程序需要，否则这些寄存器也不会恢复。

**例子**：有进程A和B。进程A在运行并使用浮点操作，在调度器切换到进程B时，进程A的浮点寄存器的内容保存到进程的线程数据结构中。但这些寄存器中的值不会立即被来自进程B的值替换。

1) 进程B在其时间片内部不执行任何浮点操作，在进程A下一次激活时，会看到CPU浮点寄存器内容与此前相同

2) 进程B执行了浮点操作，该事实报告给内核，它会用来自线程数据结构的适当值填充寄存器。因此，只有在需要的情况下，内核才保存和恢复浮点寄存器内容

## 完全公平调度类

核心调度器必须知道的有关完全公平调度器的所有信息，都包含在fair_sched_class中：

```c
/*
 * All the scheduling class methods:
 */
const struct sched_class fair_sched_class = {
	.next			= &idle_sched_class,
	.enqueue_task		= enqueue_task_fair,
	.dequeue_task		= dequeue_task_fair,
	.yield_task		= yield_task_fair,
	.yield_to_task		= yield_to_task_fair,

	.check_preempt_curr	= check_preempt_wakeup,

	.pick_next_task		= pick_next_task_fair,

	.put_prev_task		= put_prev_task_fair,
	.set_next_task          = set_next_task_fair,

#ifdef CONFIG_SMP
	.select_task_rq		= select_task_rq_fair,
	.migrate_task_rq	= migrate_task_rq_fair,

	.rq_online		= rq_online_fair,
	.rq_offline		= rq_offline_fair,

	.task_dead		= task_dead_fair,
	.set_cpus_allowed	= set_cpus_allowed_common,
#endif

	.task_tick		= task_tick_fair,
	.task_fork		= task_fork_fair,

	.prio_changed		= prio_changed_fair,
	.switched_from		= switched_from_fair,
	.switched_to		= switched_to_fair,

	.get_rr_interval	= get_rr_interval_fair,

	.update_curr		= update_curr_fair,

#ifdef CONFIG_FAIR_GROUP_SCHED
	.task_change_group	= task_change_group_fair,
#endif

#ifdef CONFIG_UCLAMP_TASK
	.uclamp_enabled		= 1,
#endif
};
```

### CFS就绪队列结构

数据结构

```c
struct cfs_rq{
  struct load_weight load;
  ubsigned long nr_running;
  
  u64 min_vruntime;
  
  struct rb_root tasks_timesline;
  struct rb_node *rb_leftmost;
  struct sched_entity *curr;
}
```

- nr_running计算队列上可运行进程的数目
- load维护所有这些进程的累积负荷值
- min_vruntime跟踪记录队列上所有进程的最小虚拟运行时间
- tasks_timesline用于在按时间排序的红黑树中管理所有进程
- curr指向当前执行进程的可调度实体

### CFS操作

CFS调度器提供的调度方法

**虚拟时钟**

完全公平调度算法依赖于虚拟时钟，用来度量等待进程在完全公平系统中所能得到的CPU时间。虚拟时间根据现存的实际时钟和每个进程相关的负荷权重推算出来

所有与虚拟时钟有关的计算都在update_curr中执行, 该函数在系统中各个不同地方调用, 包括周期性调度器在内。update_curr的流程如下

1. 首先计算进程当前时间与上次启动时间的差值
2. 通过负荷权重和当前时间模拟出进程的虚拟运行时钟
3. 重新设置cfs的min_vruntime保持其单调性

**红黑树排序**

完全公平调度器的真正关键点是，红黑树排序过程是根据下列键进行的：

```c
static inline s64 entity_key(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
    return se->vruntime - cfs_rq->min_vruntime;
}
```

键值较小的结点, 在CFS红黑树中排序的位置就越靠左, 因此也更快地被调度. 用这种方法, 内核实现了下面两种对立的机制

1. 在程序运行时, 其vruntime稳定地增加, 他在红黑树中总是向右移动的.

   因为越重要的进程vruntime增加的越慢, 因此他们向右移动的速度也越慢, 这样其被调度的机会要大于次要进程, 这刚好是我们需要的

2. 如果进程进入睡眠, 则其vruntime保持不变. 因为每个队列min_vruntime同时会单调增加, 那么当进程从睡眠中苏醒, 在红黑树中的位置会更靠左, 因为其键值相对来说变得更小了.

**延迟跟踪**

良好的调度延迟，保证每个可运行的进程都应该至少执行一次的某个时间间隔，默认是20毫秒。

第二个参数控制在一个延迟周期中处理的最大活动进程数目。如果活动进程的数目超过该上限，则延迟周期也成比例的线性扩展。

### 队列操作

有两个函数可用来增删就绪队列的成员

- enqueue_task_fair
- dequeue_task_fair

**enqueue_task_fair**的执行流程

- 如果通过struct sched_entity的on_rq成员判断进程已经在就绪队列上, 则无事可做.
- 否则, 具体的工作委托给enqueue_entity完成, 其中内核会借机用update_curr更新统计量 在enqueue_entity内部如果需要会调用__enqueue_entity将进程插入到CFS红黑树中合适的结点


enqueue_entity插入进程

- 更新一些统计统计量, update_curr, update_cfs_shares等
- 如果进程此前是在睡眠状态, 则调用place_entity中首先会调整进程的虚拟运行时间
- 最后如果进程最近在运行, 其虚拟运行时间仍然有效, 那么则直接用__enqueue_entity加入到红黑树

**dequeue_task_fair**出队操作

dequeue_task_fair函数在完成睡眠等情况下调度, 将任务从就绪队列中移除。其执行的过程正好跟enqueue_task_fair的思路相同, 只是操作刚好相反

dequeue_task_fair的执行流程如下

- 如果通过struct sched_entity的on_rq成员判断进程已经在就绪队列上, 则无事可做.
- 否则, 具体的工作委托给dequeue_entity完成, 其中内核会借机用update_curr更新统计量 。在enqueue_entity内部如果需要会调用__dequeue_entity将进程插入到CFS红黑树中合适的结点

### 选择下一个进程

选择下一个将要运行的进程有pick_next_task_fair执行

基本流程是

1. !cfs_rq->nr_running -=> goto idle：如果nr_running计数器为0, 当前队列上没有可运行进程, 则需要调度idle进程
2. pick_next_entity：如果树最左边的进程可用，可以使用函数first_fair确定，然后用_pick_next_entity从红黑树中提取sched_entity实例
3. se = pick_next_entity(cfs_rq, NULL)：选出下一个可执行调度实体
4. set_next_entity(cfs_rq, se)：set_next_entity会调用__dequeue_entity把选中的进程从红黑树移除，并更新红黑树



### 处理周期性调度器

在处理周期调度时，周期差值很重要，形式上由task_tick_fair负责，实际工作由entity_tick完成。

基本流程是

1. update_curr：更新统计量。如果队列nr_running计数器表明队列上可运行的进程少于两个，则实际上无事可做。如果某个进程应该被抢占，那么至少需要有另一个进程能够抢占它。
2. 可运行进程多于一个，check_preemt_tick做出决策。该函数的目的在于，确保没有哪个进程能够比延迟周期中确定的份额运行更长
3. 如果进程运行时间比期望的时间间隔长，那么通过resched_task发出重调度请求。这会在task_struct设置TIF_NEED_RESCHED标志。核心调度器会在下一个适当时机发起重调度。

### 唤醒抢占

内核使用check_preempt_curr看看是否新进程可以抢占当前运行的进程。新唤醒的进程不必一定由完全公平调度器处理。如果新进程是一个实时进程，则会立即请求重调度，因为实时进程总会抢占CFS进程

当运行进程被新进程抢占时，内核确保被抢占者至少已经运行了某一最小时间限额。

增加的时间缓冲确保了进程不至于切换得太频繁，避免花费过多的时间用于上下文切换，而非实际工作。

### 处理新进程

对完全公平调度器需要考虑的最后一个操作是创建新进程时调用的挂钩函数：task_new_fair。

如果父进程的虚拟运行时间小于子进程的虚拟运行时间，则意味着父进程将在子进程之前调度运行。虚拟运行时间比较小，则在红黑树中的位置比较靠左。如果子进程应该在父进程之前运行，则二者的虚拟运算时间需要换过来

然后子进程按常规加入就绪队列，并请求重调度。

## 实时调度器

### 性质

实时进程与普通进程有一个根本的不同之处：如果系统汇总有一个实时进程且可运行，那么调度器总是会选中它运行，除非有另一个优先级更高的实时进程。

两种实时进程

- 循环进程：有时间片，其值在进程运行时会减少，就像普通进程。在所有的时间段都到期后，则该值重置为初始值，而进程则置于队列的末尾。这确保了再有几个优先级相同的SCHED_RR进程的情况下，它们总是依次执行。
- 先进先出进程：没有时间片，在被调度器选择执行后，可以运行任意长时间。不进入睡眠即可。

调度类定义

```c
const struct sched_class rt_sched_class = {
	.next = &fair_sched_class,
	.enqueue_task = enqueue_task_rt,
	.dequeue_task = dequeue_task_rt,
	.yield_task = yield_task_rt,
	.check_preempt_curr = check_preempt_curr_rt,
	.pick_next_task = pick_next_task_rt,
	.put_prev_task = put_prev_task_rt,
	.set_curr_task = set_curr_task_rt,
	.task_tick = task_tick_rt,
};
```

核心调度器的就绪队列也包括用于实时进程的子就绪队列，是一个嵌入的struct rt_rq实例。就绪队列非常简单，链表就足够了。

```c
struct rq {
	...
	t_rq rt;
	...
}
```

具有相同优先级的所有实时进程都保存在一个链表中，表头为active.queue[prio]，而active.bitmap位图中的每个比特位对应于一个链表。

凡包含进程的链表，对应的比特位则置位。如果链表中没有进程，则对应的比特位不置位

![img](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/811006-20181118163017892-227738262.png?raw=true)

实时调度器类中对应于update_cur的是update_curr_rt，该函数将当前进程在CPU上执行花费的时间记录在sum_exec_runtime中。所有计算的单位都是实际时间，不需要虚拟时间。

### 调度器操作

进程的入队和离队都比较简单。只需以p -> prio为索引访问queue数组queue[p->prio]，即可获得正确的链表，将进程加入链表或从链表删除即可。

放置选择下一个将执行的进程pick_next_task_rt

1. sched_find_first_bit：可以找到active.bitmap中第一个置位的比特位，这意味高的实时优先级(对应较低的内核优先级值)，因此在较低的实时优先级之前处理。
2. 将进程从队列取出
3. 将se.exec_start设置为就绪队列的当前实际时钟值

为将进程转换为实时进程，必须使用sched_setscheduler系统调用

- 使用deactivate_task将进程从当前队列移除
- 在task_strcut中设置实时优先级和调度类
- 重新激活进程

如果进程此时不在任何就绪队列上，那么只需要设置调度类和新的优先级数值。停止进程活动和重激活则是不必要的。

## 调度器增强

在Linux中，除了支持多个CPU之外，内核也提供其他几种与调度相关的增强功能。

### SMP调度

多处理器系统上，内核必须考虑几个额外的问题，以确保良好的调度。

- CPU负荷必须尽可能公平地在所有的处理器上共享。如果一个处理器负责3个并发的应用程序，而另一个只能处理空闲进程，那是没有意义的。
- 进程与系统中某些处理器的亲合性（affinity）必须是可设置的。例如在4个CPU系统中，可以将计算密集型应用程序绑定到前3个CPU，而剩余的（交互式）进程则在第4个CPU上运行。
- 内核必须能够将进程从一个CPU迁移到另一个。但该选项必须谨慎使用，因为它会严重危害性能。在小型SMP系统上CPU高速缓存是最大的问题。对于真正大型系统， CPU与迁移进程此前使用的物理内存距离可能有若干米，因此对该进程内存的访问代价高昂。

**数据结构**

在SMP系统上，每个调度器增加两个额外函数

- load_balance：允许从最忙的就绪队列分配多个进程到当前CPU，但移动的负荷不能比max_load_move更多
- move_one_task：使用iter_move_task从最忙碌的就绪队列移出一个进程，迁移到当前CPU的就绪队列

**负载均衡处理**

在SMP系统，周期性调度器函数scheduler_tick按上文所述完成所有系统都需要的任务之后，会调用trigger_load_balance函数。这会引发SCHEDULE_SOFTIRQ软中断softIRQ。该中断确保会在适当的时机执行run_rebalance_domain。![img](https://github.com/orangehaswing/Linux_Kernel_Architecture_Note/blob/master/%E8%BF%9B%E7%A8%8B/resources/811006-20181125225152806-1295238894.png?raw=true)

为执行重新均衡操作，内核需要在就绪队列增加额外的字段：

```
struct rq{
  #ifdef CONFIG_SMP
  		struct sched_domain *sd;
  		/* 用于主动均衡 */
  		int active_balance;
  		int push_cpu;
  		/* 该就绪队列的CPU */
  		int cpu;
 		struct task_struct *migration_thread;
 		struct list_head migration_queue;
 #endif 		
 ... 
}
```

 内核为每个就绪队列提供一个迁移线程，可以接收迁移请求。但如果进程被限制在某一特定的CPU集合上，而不能在当前执行的CPU上继续运行，也可能出现重新均衡的操作。

内核试图周期性地均衡就绪队列，但如果对某个就绪队列效果不佳，则必须使用主动均衡。

此外，所有的就绪队列组织为调度域。这可以将物理上邻近或共享告诉缓存的CPU集群起来，应优先选择在这些CPU之间迁移进程。

**load_balance函数**

该函数会检测在上一次重新均衡操作之后是否已经过去了足够的时间，在必要的情况下通过调用load_balance发起一轮新的重新均衡操作。

1. find_busiest_cpu：标识出哪个队列工作量最大。迭代所有处理器的队列，比较其负荷权重，最忙的队列就是最后找到的负荷值最大的队列
2. 标识出一个非常繁忙的队列之后，如果至少有一个进程在该队列上执行(否则负载均衡没有意义)，则使用move_tasks将该队列中适当数目的进程迁移到当前队列
3. move_tasks函数调用特定于调度器类的load_balance方法

*注意：在选择被迁移的进程时，内核必须确保所述的进程*

- *目前没有运行或刚结束运行。因为对运行进程而言，CPU告诉缓存充满了进程的数据，迁移该进程则完全抵消了高速缓存带来的好处*
- *根据其CPU亲和性，可以在与当前队列相关的处理器上执行*

如果均衡操作失败(**例如**：远程队列上所有进程都有较高的内核内部优先级值，即较低的nice值)，那么将唤醒负责最忙的就绪队列的迁移线程。

**迁移线程**

目的：1. 用于完成自发调度器的迁移请求；2. 用于实现主动均衡迁移线程是一个执行migration_thread的内核线程

内部流程

migration_thread内部是一个无限循环，在无事可做时进入睡眠状态。

1. 检测是否需要主动均衡
2. 若需要，调用active_load_balance：试图从当前就绪队列移出一个进程，且移至发起主动均衡请求CPU的就绪队列。
3. 使用move_one_task完成该工作，对所有的调度器类，分别调用特定于调度器类的move_one_task函数，直到其中一个成功。
4. 完成主动负载均衡之后，迁移线程会检测migrate_req链表中是否有来自调度器的待决迁移请求
5. 如果没有，则线程发出重调度请求。否则，用_migrate_task完成相关请求，直接移出所要求的进程，而不再与调度器类进一步交互

**核心调度器的改变**

SMP系统还需要对核心调度器的现存方法做一些修改。

- 在用exec系统调用启动一个新进程时，是调度器跨越CPU移动该进程的一个良好时机。调用sched_balance_self挑选当前负荷最少的CPU。如果不是当前CPU，那么使用sched_migrate_task，向迁移线程发送一个迁移请求
- 完全公平调度器的调度粒度与CPU树木成比例。系统中处理器越多，可以采用的调度粒度就越大。

### 调度域和控制组

调度器并不直接与进程交互，而是处理可调度实体。这可以实现组调度：进程置于不同的组中，调度器首先在这些组之间保证公平，然后在组中的所有进程之间保证公平。

把进程按用户分组不是唯一的做法。内核还提供了控制组(control group)，该特性使得通过特殊文件系统cgroups可以创建任意的进程集合，甚至可以分为多个层次

### 内核抢占和低延迟工作

**内核抢占**

在编译内核时启用对内核抢占的支持，则可以解决这些问题。如果高优先级进程有事情需要完成，那么在启动情况下，不仅用户空间应用程序可以被中断，内核也可以被中断。但是内核抢占和用户层被其他进程抢占是两个不同的概念

考虑临界区内部工作的内核被抢占的情形，需要确定内核当前是否处于一个可以被中断的位置。如果preempt_count为零，则内核可以被中断，否则不行。每次内核进入重要区域，需要禁止抢占时，都会调用inc_preempt_count。在退出该区域时，则调用dec_preempt_count将抢占计数器的值减1。

由于内核可能通过不同线路进入某些重要的区域，特别是嵌套的线路，因此preempt_count使用简单的布尔变量是不够的。在陆续进入多个临界区时，在内核再次启用抢占之前，必须确认已经离开所有的临界区。

**低延迟**

基本上，内核中耗时长的操作不应该完全占据整个系统。相反，它们应该不时地检测是否有另一个进程变为可运行，并在必要的情况下调用调度器选择相应的进程运行。该机制不依赖于内核抢占，即使内核编译时未指定支持抢占，也能够降低延迟。



















